---
layout: post
title:  "Coding a Neuron with Numpy"
author: luc
categories: [ TDs, Deep Learning, Neural Network, Numpy, Keras ]
image_folder: /assets/images/post_coding_a_neuron_with_numpy/
image: assets/images/post_coding_a_neuron_with_numpy/index_img/network.jpg
image_index: assets/images/post_coding_a_neuron_with_numpy/index_img/network.jpg
tags: [featured]
toc: true
order: 7

---

# Coding a Neuron with Numpy

<!-- <h3> Un <a href="http://playground.tensorflow.org/#activation=linear&regularization=L1&batchSize=29&dataset=gauss&regDataset=reg-plane&learningRate=0.001&regularizationRate=0.003&noise=15&networkShape=1&seed=0.37334&showTestData=true&discretize=false&percTrainData=50&x=false&y=false&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false">lien sympathique</a> pour s'amuser avec différentes architectures de réseau de neurones </h3> -->


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

# A scalar


```python
scalar = 4
scalar2 = np.array(4)
scalar, scalar2, scalar2.shape
```




    (4, array(4), ())



# A vector

## Creation


```python
vector_1D = np.array([4])
vector_1D_of_multiple_elements = np.array([1,2,3,4,5,6])

print(vector_1D, vector_1D_of_multiple_elements)
print(vector_1D.shape, vector_1D_of_multiple_elements.shape)
print(vector_1D.ndim, vector_1D_of_multiple_elements.ndim)

# Number of elements in the array
print(vector_1D.size, vector_1D_of_multiple_elements.size)
```

    [4] [1 2 3 4 5 6]
    (1,) (6,)
    1 1
    1 6


## Transpose vector


```python
(vector_1D_of_multiple_elements.T,
 vector_1D_of_multiple_elements.T.shape) # same thing (in terms of representation)
```




    (array([1, 2, 3, 4, 5, 6]), (6,))




```python
(vector_1D_of_multiple_elements, 
vector_1D_of_multiple_elements.shape,
vector_1D_of_multiple_elements.ndim)
```




    (array([1, 2, 3, 4, 5, 6]), (6,), 1)



# A matrix

## creation


```python
matrix = np.array( [ 
            [ 1,2,3], 
            [ 4,5,6]
          ])
```


```python
matrix, matrix.shape, matrix.ndim, matrix.size
```




    (array([[1, 2, 3],
            [4, 5, 6]]),
     (2, 3),
     2,
     6)



## transpose matrix


```python
matrix.T, matrix.T.shape, matrix.T.ndim, matrix.size
```




    (array([[1, 4],
            [2, 5],
            [3, 6]]),
     (3, 2),
     2,
     6)



# Re-shape a vector or matrix


```python
vector_1D.size
```




    1




```python
vector_1D.reshape((1,1))
```




    array([[4]])




```python
vector_1D.reshape((1,1,1))
```




    array([[[4]]])




```python
vector_1D_of_multiple_elements.size
```




    6




```python
vector_1D_of_multiple_elements.reshape(3,2)
```




    array([[1, 2],
           [3, 4],
           [5, 6]])




```python
vector_1D_of_multiple_elements.reshape(2, 3)
```




    array([[1, 2, 3],
           [4, 5, 6]])



## pd.DataFrame


```python
pd.DataFrame(vector_1D)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.DataFrame(vector_1D_of_multiple_elements)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.DataFrame(matrix)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.DataFrame(vector_1D_of_multiple_elements) 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.DataFrame(vector_1D_of_multiple_elements.T)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.DataFrame(vector_1D_of_multiple_elements).shape
```




    (6, 1)




```python
pd.DataFrame(vector_1D_of_multiple_elements).T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



# Vector "as" matrix

In linear algebra, a **column vector** or **column matrix** is an **m × 1 matrix**, that is, a **matrix consisting of a single column of m elements**


```python
as_matrix = vector_1D_of_multiple_elements.reshape(6, 1) # 6 rows, 1 column
```


```python
pd.DataFrame(as_matrix)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



Similarly, a row vector or row matrix is a 1 × m matrix, that is, a matrix consisting of a single row of m elements


```python
as_matrix = vector_1D_of_multiple_elements.reshape(1, 6) # 1 column, 6 rows
```


```python
pd.DataFrame(as_matrix)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



# Dot Product (produit scalaire)


```python
vector1 = np.array([1,2,3,4,5])
vector2 = np.array([2,4])
```


```python
try:
    np.dot( vector1, vector2 )
except:
    print("Not the same number of elements to perform a dot product (must be aligned)")
```

    Not the same number of elements to perform a dot product (must be aligned)



```python
vector2 = np.array([2,4,1,1,1])
```


```python
np.dot(vector1, vector2) # 1*2 + 2*4 + 3*1 + 4*1 + 5*1
```




    22




```python
try:
    np.dot(vector1.reshape(1,5), vector2.reshape(1,5))
except:
    print("Again, not aligned because in maths you do the transpose of the vector1 ")
```

    Again, not aligned because in maths you do the transpose of the vector1 


<img src="{{page.image_folder}}dot product.png" width="100%">


```python
# this works
result_dot_product = np.dot(vector1.reshape(1,5), vector2.reshape(1,5).T)
# this works
result_matrix_product = np.matmul(vector1.reshape(1,5), vector2.reshape(1,5).T)
# same result
result_dot_product, result_matrix_product
```




    (array([[22]]), array([[22]]))



# elementwise multiplication (Hadamard Product on Matrices (or column/row vector)
To not confuse with dot product:


```python
vector1 * vector2 # vector of element multiplications
```




    array([2, 8, 3, 4, 5])




```python
# same result
np.multiply(vector1, vector2)
```




    array([2, 8, 3, 4, 5])




```python
np.multiply(vector1.reshape(1,5), vector2.reshape(1,5))
```




    array([[2, 8, 3, 4, 5]])



# Broadcasting rules numpy

refers to how numpy **treats arrays** with **different shapes** during **arithmetic operations**.<br>
Subject to certain constraints, the **smaller array is “broadcast” across the larger array** so that they have compatible shapes!

* Same shapes, no problem:


```python
vector1 = np.array([1,2,3])
vector2 = np.array([1,2,3])

vector1 + vector2
```




    array([2, 4, 6])



* different shapes, what to do ? 

<u>**Example1:**</u>


```python
vector1 = np.array([1])
vector2 = np.array([1,2,3])
vector1.shape, vector2.shape
```




    ((1,), (3,))


<img src="{{page.image_folder}}Example1-broadcasting.png" align="left" width="100%">


```python
vector1 + vector2
```




    array([2, 3, 4])



This is the same thing as:


```python
vector1_transformed = np.tile(vector1, reps=(1,3))
print( vector1_transformed )

vector1_transformed + vector2
```

    [[1 1 1]]





    array([[2, 3, 4]])



> from https://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting:
* If the arrays do not have the same rank (number of dimensions), **prepend the shape of the lower rank array** with **1s until both shapes have the same length**.
* The two arrays **are said to be compatible** in **a** dimension if they have the **same size** in the dimension, or if one of the arrays **has size 1** in **that** dimension.
* The arrays can be broadcast together if they **are compatible in all dimensions**.


After broadcasting:
* After broadcasting, each array **behaves as if it had shape equal to the elementwise maximum of shapes** of the two input arrays.
* In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves **as if it were copied along that dimension**.


```python
vector1 = np.array([[1]])
vector2 = np.array([1,2,3])
vector1.shape, vector2.shape
```




    ((1, 1), (3,))



prepending ones, the last dimension of the array will be "strecthed" so the number of elements match with the one of the first array


```python
vector1 + vector2
```




    array([[2, 3, 4]])



<u>**Example2:**</u>


```python
vector1 = np.array([[1], [2]]) # 2 rows, 1 column
vector2 = np.array([1,2,3]) # 1D vector
vector1.shape, vector2.shape
```




    ((2, 1), (3,))


<img src="{{page.image_folder}}/Example2-broadcasting.png" align="left" width="100%">



1. 2 rows in `vector1`, `vector2` does not have "rows dimension", prepended a 1 to create a new dimension
2. then `vector2` is stretched on its new dim so to have 2 elements, as `vector1` 
3. while `vector1` has last dimensions (columns) strecthed to have 3 elements, as `vector2`


```python
vector1 + vector2 
```




    array([[2, 3, 4],
           [3, 4, 5]])



This is the same as:


```python
vector2
```




    array([1, 2, 3])




```python
np.tile(vector2, reps=(2, 1))
```




    array([[1, 2, 3],
           [1, 2, 3]])




```python
np.tile(vector1, reps=(1, 3))
```




    array([[1, 1, 1],
           [2, 2, 2]])



<u>**Example3:**</u>


```python
vector1 = np.array([[1, 4, 5], [2, 2, 5]]) # 2 rows, 1 column
vector2 = np.array([1, 2]) # 1D vector
vector1.shape, vector2.shape
```




    ((2, 3), (2,))




```python
try:
    vector1 + vector2
except Exception as e:
    print("After prepending with 1 the vector2, impossible to match 2 to 3:\n{}".format(e))
```

    After prepending with 1 the vector2, impossible to match 2 to 3:
    operands could not be broadcast together with shapes (2,3) (2,) 


Another example:

<img src="{{page.image_folder}}/Example3-broadcasting.png" align="left" width="100%">



# Finding the parameters in a simple linear regression case

## The data


```python
%matplotlib inline
```


```python
x = np.linspace(0, 100, 100)
y = 8*x + np.random.normal(x, 100) # y = 8*x + epsilon with epsilon ~ N(0,1)
plt.scatter(x, y)
plt.show()
```


<img src="{{page.image_folder}}output_80_0.png" align="left" width="100%">


How to find the coefficient $$\beta$$s (here the intercept $$\beta$$0 and the slope $$\beta$$1) in order to have the best fitting (simple) linear model ? 

## The plotting function


```python
def plotting(beta0, beta1):
    plt.scatter(x_scaled_and_centered, y)
    plt.plot(x_scaled_and_centered, lm.intercept_ + lm.coef_ * x_scaled_and_centered, color='r')
```

## Using OLS


```python
from sklearn.linear_model import LinearRegression
# adding one dimension to the x (to have a feature matrix notation,
# although x is only 1 feature,
# which then can be apparented as a column vector
lm = LinearRegression().fit(x[:, np.newaxis], y)
lm.intercept_, lm.coef_
```




    (37.18580177143173, array([8.22801749]))



With standardization before:


```python
from sklearn.preprocessing import StandardScaler
x_scaled_and_centered = StandardScaler().fit_transform(x[:, np.newaxis])
lm = LinearRegression(fit_intercept=True).fit(x_scaled_and_centerd, y)
lm.intercept_, lm.coef_
```




    (448.5866764712714, array([239.90962559]))




```python
plotting(lm.intercept_, lm.coef_)
```


<img src="{{page.image_folder}}output_88_0.png" align="left" width="100%">


## Using a self-made (definitely non-optimised) algorithm


```python
def algo_simple_linreg(x, y):
    """A self-made, definitely non-optimised algorithm to find the best alpha and beta values:"""
    from sklearn.metrics import mean_squared_error
    MSE = {}
    for beta0 in np.linspace(-5000, 5000, 100):
        for beta1 in np.linspace(-5000, 5000, 100):
            model = lambda x: beta0 + beta1*x
            mse = mean_squared_error( model(x), y)
            MSE[(beta0, beta1)] = mse
    return MSE
```


```python
MSE = algo_simple_linreg(x_scaled_and_centered, y)
```


```python
params = pd.Series(MSE).unstack()
```


```python
import seaborn as sns
ax = plt.subplot(111)
sns.heatmap(params, cmap="coolwarm", ax=ax)
```




    <AxesSubplot:>




<img src="{{page.image_folder}}output_93_1.png" align="left" width="100%">



```python
params.stack().idxmin()
```




    (454.54545454545496, 252.52525252525265)




```python
plotting(*params.stack().idxmin())
```


<img src="{{page.image_folder}}output_95_0.png" align="left" width="100%">


## Using One neuron 🤓

###  Definition

2 nice definitions i like, just stolen from [there](https://people.minesparis.psl.eu/fabien.moutarde/ES_MachineLearning/Slides/MLP-NeuralNetworks_course_2pp.pdf)

> A **processing “unit”** applying a **simple operation to its inputs**, and which can be **“connected” to others** to build a **network** able to realize any input-output function 

> “Usual” definition: a “unit” computing a **weighted sum of its inputs**, that can add a constant term, and **apply some non-linearity (sigmoïd, ReLU, ...)**

<img src="{{page.image_folder}}1neuron.png" align="left" width="100%">

## Behavior formulas

From the last definition of what a neuron is we get:

From the last definition of what a neuron is we get:

1. <span style="color: red;">Weighted sum of its inputs</span> and <span style="color: blue;">can add a constant term<span>.

$$$ f(x_i) =  \color{red}{\sum_{i=1}^{p}{w_i x_i}}   + \color{blue}{cst}$$$

2. Apply some <span style="color: green;">non-linearity function g</span>:<br>
    Example: sigmoid function (g is Sigmoid)
    $$$ g(z) = Sigmoid(z) = \color{green}{\frac{1}{1+e^{-z}}} $$$

**Then the output of the neuron is:**
$$$ y_i = ( g \circ f ) (x) = g(f(x)) = \color{green}{\frac{1}{1+e^{-\color{red}{\sum_{i=1}^{p}{w_i x_i}}   + \color{blue}{cst}}}} $$$

Seems that **<span style='color: red;'><u>1.</u></span>** look very similar to a simple linear regression formula !<br>
- The **weights** $$w_i$$ can be seen as the **coefficients** of a linear regression.
- The $$x_i$$ as the **features** of **one** data point (one **row vector** then i.e. **one line of a matrix** or one observation in a **dataframe** !). There is $$p$$ features for one input vector here using the former notation.
- The output $$y_i$$ is a scalar, that is, the output for one input vector of features $$i$$

We can rewrite this formula in **vector notation**, so we could scale this to **multiple input vectors**.

$$$  Y = (g \circ f) (X) = g( X W + B ) $$$

or maybe using the indices so it is a little bit clearer

$$$  Y_{k,1} = (g \circ f) (X_{k,p}) = g( X_{k,p} W_{p,k} + B_{k,1} ) $$$

Where $$X$$ is a **row vector of p features** (or a **matrix of n row vectors of p features**).<br>
This notation is useful as it could be used for one single input, or many.
- if **one input row vector** is passed, then, it is a **simple dot product** between this vector and a column **weights vector** occur, forming one scalar output $$Y_{1,1}$$.
- if multiple inputs are being passed (size $$k x p$$), then W is a matrix of size $$p x k$$, so that Y has k output (one for each input) and that each feature of x is multiplied by its corresponding feature in W, forming finally a vector of outputs $$Y_{k,1}$$.

Let's see the simple linear regression as a specification of multiple linear regression: $$W_{k,p}$$ for k inputs of p features


```python
x = x[:, np.newaxis] # to set x as a matrix of row vectors of 1 feature 
```


```python
W = np.random.random(size=tuple(reversed(x.shape))) # all are between 0 and 1 for stability at first
```

the bias term:


```python
B = np.random.random(size=(x.shape[0], 1))
```

## Loss and Risk function

Remembered the cost function ?<br>
Let's take a **quadratic loss** as it is **nicely differentiable**,<br>

Let's write: $$$ z = (g \circ f) $$$

then:

$$$ L(y_i, \hat{y}_i) = L(y_i, z(x_i)) = (y_i - z(x_i))^2 $$$

Then in matrix notation:

$$$ L(Y_{k,1}, \hat{Y}_{k,1}) = L(Y_{k,1}, z(X_{k,p}) =  (Y_{k,1} - z(X_{k,p}))^2 $$$


Hence the result is a vector of loss for each output.

The cost function is the **expected loss value**, if we use the quadratic loss it then becomes the **Mean Squared error**.

$$$ MSE = \sum_{i=1}^{n}{ ( y_i - z(x_i) )^2}$$$

and in matrix notation:

$$$ MSE = E[L(Y_{k,1}, \hat{Y}_{k,1})]= E[ (Y_{k,1} - z(X_{k,p}))^2 ) ] $$$

## Backpropagation

At first the weights (coefficients for a linear regression here) are chosen **randomly**.<br>
Of course, if we knew them before, why would we use an algorithm ? :P

We are going to use **Gradient descent**: a **first-order** iterative optimization algorithm for **finding a local minimum of a differentiable function**. We want to minimize the errors produces, we will perform the gradient descent of the loss function. It implies computing the derivative of the loss function w.r.t. the weights. The **quadratic loss function** is then a good choice here as it is differentiable.

Computing the gradient of the **loss function** with respect to the **weights** enable us to later find the direction in the weight/parameter space that **minimizes the loss**.

This derivative can be done in 2 different ways:
- each iteration can use **one input vector**. Each of the weights will be updated computing the derivative on the loss function w.r.t. the weights for **that single input vector** that had been passed forward to compute the output and so the errors, this is named: **stochastic gradient descent**.
- or each iteration can use a **batch of multiple vectors** (extreme case is using a bach equaling to the training set, the whole data available, that is, **k row vectors**) to compute the **expected loss value for that batch of inputs**, this is named: **batch gradient descent**. This means that each weight will be updating by the same quantity **meaned** over the grouped information from the predictions errors drawn from passing **k input vectors**.  

Once computed, the gradient points **uphill** (maximize the loss), so we need to update the weights taking the opposite direction. Also we will carefully take each update a **little step in this same direction** by using the (negation of the) derivative by a coefficient also called **learning rate**: since it influences to what extent **newly acquired information overrides old information** (wikipedia always gives the best quote).


<img src="{{page.image_folder}}Sgd-VS-BATCH.png" align="left" width="100%">



How to compute such gradient w.r.t. to the weights ? We use the **chain rule** !

> from wikipedia: Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x.<br> As put by George F. Simmons: "if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man."m

We are going to see how vary the **prediction errors** by making a **change in the weight space** (w.r.t. to each weight = gradient).<br>
Here we face a **composite function**, as computing such derivative w.r.t one weight implies (using the chain rule):
- to first derive w.r.t the output of the activation function,
- then see how the output of the activation function changes w.r.t. the variable before the activation function (weighted inputs sum)
- then w.r.t. to the weight itself.


<img src="{{page.image_folder}}Gradient-descent.png" align="left" width="100%">


# Recap 


<img src="{{page.image_folder}}1neuron.png" align="left" width="100%">


# Time to implement it 

which activation are we going to use ?


```python
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 100, 800)
y = 8*x + np.random.normal(x, 200) # y = 8*x + epsilon with epsilon ~ N(0,1)
plt.scatter(x, y)
plt.show()
```


<img src="{{page.image_folder}}output_143_0.png" align="left" width="100%">



```python
x = x[:, np.newaxis] # to set x as a matrix of row vectors of 1 feature b
```


```python
def split_n_batch_indexes(X, nb_chunks=5):
    import numpy as np
    indexes = np.arange(len(X))
    shuffled_indexes = np.random.shuffle(indexes)
    return np.array_split(indexes, nb_chunks)
```


```python
class Neuron:
    """ Implementation of a single Neuron accepting multiple inputs """
    def __init__(self, X, y, nb_epochs=100, nb_batches=5, 
                 learning_rate=0.01, activation="linear"):
        self.X, self.y = X, y
        # random weights
        self.W = np.random.random(size=(X.shape[1], 1))
        # random bias (only one as only one output)
        self.B = np.random.random(size=(1, 1))
        # number of epochs
        self.nb_epochs = nb_epochs
        # number of batches 
        self.nb_batches = nb_batches
        # learning rate
        self.learning_rate = learning_rate
        # activation
        if activation=="linear":
            self.activation = lambda x: x
            self.derivative = lambda x: 1
        # records
        self.records = {}
                
    def forward_pass(self, is_batch):
        """ a single forward pass to compute a prediction """
        self.y_pred_batch = self.activation( self.X[is_batch] @ self.W + self.B)
        self.y_pred = self.activation( self.X @ self.W + self.B)
        
    def compute_mse_on_whole_dataset(self):
        """ return the mean squared errors on whole training set"""
        self.mse = np.mean((self.y - self.y_pred)**2)
        
    def backpropagation(self, is_batch):
        """ compute the gradient of the errors with respect to 
        the weights using the chain rule """
        # gradient of the errors w.r.t the output/prediction
        dE_dout = 2*(self.y_pred_batch - self.y[is_batch, np.newaxis])
        # gradient of the prediction w.r.t before the activ. func
        dout_dz = self.derivative(self.X[is_batch]) #1 for linear
        # gradient of z w.r.t the weight
        dz_dw = self.X[is_batch]
        # final gradient w.r.t the weights:
        self.dE_dw = dE_dout * dout_dz * dz_dw
        # for the biases (only last part change)
        self.dE_db = dE_dout * dout_dz * 1        
    
    def update_weights_and_biases(self):
        dE_dw = self.dE_dw.mean(axis=0)[:, np.newaxis]
        dE_db = self.dE_db.mean(axis=0)[:, np.newaxis]
        
        self.W = self.W - self.learning_rate * dE_dw
        self.B = self.B - self.learning_rate * dE_db


    def predict(self, X_test):
        """ same as forward pass, just provide our own X"""
        return self.activation( X_test @ self.W + self.B)
    
    def run(self):
        """ learn iteratively:
            - an iteration is a single forward and backward pass
            - an epoch is consumed when all the inputs from the 
              dataset have been used for updating the weight and 
              biases """
        # epochs
        for i in range(1, self.nb_epochs):
            # batches: 
            for batch_i, indices in enumerate(
                    split_n_batch_indexes(self.X, self.nb_batches)):
                self.forward_pass(indices)
                self.compute_mse_on_whole_dataset()
                self.backpropagation(indices)
                self.update_weights_and_biases()
                self.records[(i, batch_i)] = [self.W, self.B, self.mse]
        return self.records
```


```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y)
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```


```python
neuron = Neuron(X_train, y_train)
neuron
```




    <__main__.Neuron at 0x12d97f9a0>




```python
records = neuron.run()
```


```python
import pandas as pd
index = pd.MultiIndex.from_tuples(records.keys())
records = pd.DataFrame(records.values(), 
            index=index,
            columns=['weights', 'bias', 'mse_train'])
records
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>weights</th>
      <th>bias</th>
      <th>mse_train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">1</th>
      <th>0</th>
      <td>[[4.443509449033136]]</td>
      <td>[[9.644383787235665]]</td>
      <td>316078.087993</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[[10.21384857348198]]</td>
      <td>[[18.956325297600568]]</td>
      <td>307829.453673</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[[13.829465261075233]]</td>
      <td>[[27.481262123908465]]</td>
      <td>299653.437801</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[[18.483634718716814]]</td>
      <td>[[35.58832870180558]]</td>
      <td>292329.983030</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[[24.778160591422168]]</td>
      <td>[[44.41565823450879]]</td>
      <td>285568.062128</td>
    </tr>
    <tr>
      <th>...</th>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th rowspan="5" valign="top">99</th>
      <th>0</th>
      <td>[[257.33337321955196]]</td>
      <td>[[457.91336158008875]]</td>
      <td>173202.046267</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[[256.7620263257349]]</td>
      <td>[[457.9678800130618]]</td>
      <td>173143.006457</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[[256.873731154326]]</td>
      <td>[[457.78357255910817]]</td>
      <td>172849.289663</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[[257.1059056991105]]</td>
      <td>[[457.45374334250926]]</td>
      <td>172906.655320</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[[257.4797445493716]]</td>
      <td>[[457.8868223084108]]</td>
      <td>173026.140104</td>
    </tr>
  </tbody>
</table>
<p>495 rows × 3 columns</p>
</div>




```python
df_weights = records.weights.apply(np.ravel).apply(pd.Series)
df_bias = records.bias.apply(np.ravel).apply(pd.Series)
df_bias.rename(columns = lambda x: "bias_{}".format(x), inplace=True)
df_weights.rename(columns = lambda x: "weights_{}".format(x), inplace=True)
df = pd.concat([df_weights, df_bias], axis=1)
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>weights_0</th>
      <th>bias_0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">1</th>
      <th>0</th>
      <td>4.443509</td>
      <td>9.644384</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.213849</td>
      <td>18.956325</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13.829465</td>
      <td>27.481262</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18.483635</td>
      <td>35.588329</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.778161</td>
      <td>44.415658</td>
    </tr>
    <tr>
      <th>...</th>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th rowspan="5" valign="top">99</th>
      <th>0</th>
      <td>257.333373</td>
      <td>457.913362</td>
    </tr>
    <tr>
      <th>1</th>
      <td>256.762026</td>
      <td>457.967880</td>
    </tr>
    <tr>
      <th>2</th>
      <td>256.873731</td>
      <td>457.783573</td>
    </tr>
    <tr>
      <th>3</th>
      <td>257.105906</td>
      <td>457.453743</td>
    </tr>
    <tr>
      <th>4</th>
      <td>257.479745</td>
      <td>457.886822</td>
    </tr>
  </tbody>
</table>
<p>495 rows × 2 columns</p>
</div>




```python
df.plot(kind="line")
```

    /Users/lucbertin/.pyenv/versions/3.8.4/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py:1235: UserWarning: FixedFormatter should only be used together with FixedLocator
      ax.set_xticklabels(xticklabels)





    <AxesSubplot:>




<img src="{{page.image_folder}}output_153_2.png" align="left" width="100%">



```python
plt.scatter(X_test, y_test)
plt.plot(X_test, neuron.predict(X_test), color='green')
```




    [<matplotlib.lines.Line2D at 0x12df02610>]




<img src="{{page.image_folder}}output_154_1.png" align="left" width="100%">



```python
%matplotlib notebook

import matplotlib.animation as animation

fig, ax = plt.subplots()
# Initial plot
x_ = np.linspace(-2, 2, 100).reshape((100,1))
# y_ = weight*x_ + bias
y_ = float(df.iloc[0, 0])*x_ + float(df.iloc[0, 1])

line, = ax.plot(x_, y_, label="Fit from the neuron")

plt.rcParams["figure.figsize"] = (4,2)
plt.ylabel("y")
plt.xlabel("X")
plt.scatter(X_train, y_train, color='red', label="Training data")
plt.scatter(X_test, y_test, color='green', label="Test data")
plt.xlim(-2, 2)
plt.legend()
plt.title("Linear regression training fit using a single neuron | perceptron")

def animate(i):
    line.set_label("Fit from the perceptron : epoch {}".format(i))
    plt.legend()
    x_ = np.linspace(-2, 2, 100).reshape((100,1))
    line.set_xdata(x_)  # update the data
    line.set_ydata( float(df.iloc[i, 0])*x_ 
                   + float(df.iloc[i, 1]))# update the data
    return line,

ani = animation.FuncAnimation(fig, animate, frames=np.arange(1, len(df)), interval=100)
plt.show()
```


    <IPython.core.display.Javascript object>



<img src="data:," width="0">


# Let's try with multiple features ;-)


```python
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
scaler  = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)
lm = LinearRegression().fit(X_train, y_train)
print( "linear regression coefficients {}".format(lm.coef_) )
```

    linear regression coefficients [-0.72097319  1.32905744  0.14098068  0.31901842 -1.9611038   2.39122944
      0.02842854 -3.10284546  2.2339583  -1.52408631 -1.94909087  1.18566169
     -3.79786679]



```python
neuron_on_boston = Neuron(X_train, y_train, 
                          learning_rate=0.1, 
                          nb_batches=1)
records_on_boston = neuron_on_boston.run()
```


```python
import pandas as pd
index = pd.MultiIndex.from_tuples(records_on_boston.keys())
df_boston = pd.DataFrame(records_on_boston.values(), 
            index=index, columns=['weights', 'bias', 'mse_train'])
df_weights = df_boston.weights.apply(np.ravel).apply(pd.Series)
df_bias = df_boston.bias.apply(np.ravel).apply(pd.Series)
df_bias.rename(columns = lambda x: "bias_{}".format(x), inplace=True)
df_weights.rename(columns = lambda x: "weights_{}".format(x), inplace=True)
df = pd.concat([df_weights, df_bias], axis=1)
```


```python
%matplotlib inline
```


```python
fig = plt.Figure(figsize=(10,6))
ax = fig.gca()
save = df.drop("bias_0", axis=1).stack().unstack(level=0).loc[0].T
save.plot(ax=ax)
ax.legend().remove()
fig.legend(loc='center', bbox_to_anchor=(1,0.5))
fig
```




<img src="{{page.image_folder}}output_161_0.png" align="left" width="100%">




```python
at_10  = save.iloc[10]
at_50 = save.iloc[50]
at_98 = save.iloc[98]
```


```python
plt.bar(x=np.arange(len(lm.coef_)), height=lm.coef_, color='red')
plt.bar(x=np.arange(len(lm.coef_)), height=at_10, color='white', alpha = 0.5)
plt.bar(x=np.arange(len(lm.coef_)), height=at_50, color='white', alpha = 0.5)
plt.bar(x=np.arange(len(lm.coef_)), height=at_98, color='white', alpha = 0.5)
```




    <BarContainer object of 13 artists>




<img src="{{page.image_folder}}output_163_1.png" align="left" width="100%">


# In Keras ? 


```python
from keras.models import Sequential
from keras.layers import Dense
from keras import optimizers
import pandas as pd
```


```python
model = Sequential()
model.add(Dense(1, input_shape=(X.shape[1],), activation='linear'))
sgd = optimizers.SGD(lr=0.02)
model.compile(loss='mean_squared_error', optimizer=sgd)
```

## A callback to store weights


```python
from keras.callbacks import LambdaCallback

weights = {}
def save_weights(epoch, logs):
    weights[epoch] = model.layers[0].get_weights()
    
keep_weights = LambdaCallback(on_epoch_end=save_weights)
```


```python
history = model.fit(x=X_train, y=y_train, 
                    batch_size=X_train.shape[0], epochs=99, 
                    validation_data=(X_test, y_test), 
                    verbose=0, callbacks=[keep_weights])
```


```python
print(history.params)
losses_ = pd.DataFrame(history.history)
losses_.plot(kind="line")
```

    {'verbose': 0, 'epochs': 99, 'steps': 1}





    <AxesSubplot:>




<img src="{{page.image_folder}}output_170_2.png" align="left" width="100%">



```python
df_weights = pd.DataFrame(weights).T
coefs_linear_reg = dict(zip(
    ["weight_{}".format(_) for _ in range(len(lm.coef_))], 
    lm.coef_ 
))
coefs_linear_reg
```




    {'weight_0': -0.720973185445283,
     'weight_1': 1.3290574386023888,
     'weight_2': 0.14098068438146516,
     'weight_3': 0.3190184199461178,
     'weight_4': -1.9611038002705747,
     'weight_5': 2.3912294381194052,
     'weight_6': 0.028428542130489065,
     'weight_7': -3.1028454604117153,
     'weight_8': 2.2339582985686506,
     'weight_9': -1.5240863072636608,
     'weight_10': -1.949090868280713,
     'weight_11': 1.1856616926433907,
     'weight_12': -3.797866790151385}




```python
fig, ax = plt.subplots(figsize=(12,8))
( df_weights[0]
     .apply(np.ravel)
     .apply(pd.Series)
     .rename(columns = lambda x: "weight_{}".format(x))
     .plot(kind='line', ax=ax) )
ax.set_xlim(0,100)
ax.legend().remove()
fig.legend(loc='center', bbox_to_anchor=(1, 0.5))
```




    <matplotlib.legend.Legend at 0x14e6d2be0>




<img src="{{page.image_folder}}output_172_1.png" align="left" width="100%">



```python
at_98  = df_weights.iloc[98,0].reshape(-1)
plt.bar(x=np.arange(len(lm.coef_)), height=lm.coef_, color='red')
plt.bar(x=np.arange(len(lm.coef_)), height=at_98, color='white', alpha = 0.8)
#history.model.get_weights()
```




    <BarContainer object of 13 artists>




<img src="{{page.image_folder}}output_173_1.png" align="left" width="100%">


# Fin.
